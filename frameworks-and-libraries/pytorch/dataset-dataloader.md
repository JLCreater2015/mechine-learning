# Dataset & Dataloader

## ğŸ–Œ 1ã€Dataset

```python
# torch.utils.data.Dataset
class Dataset(object):
    """An abstract class representing a Dataset.

    All other datasets should subclass it. All subclasses should override
    ``__len__``, that provides the size of the dataset, and ``__getitem__``,
    supporting integer indexing in range from 0 to len(self) exclusive.
    """

    def __getitem__(self, index):
        raise NotImplementedError

    def __len__(self):
        raise NotImplementedError

    def __add__(self, other):
        return ConcatDataset([self, other])
```

**`torch.utils.data.Dataset`**æ˜¯ä¸€ä¸ªè¡¨ç¤ºæ•°æ®é›†çš„æŠ½è±¡ç±»ã€‚ä»»ä½•è‡ªå®šä¹‰çš„æ•°æ®é›†éƒ½éœ€è¦ç»§æ‰¿è¿™ä¸ªç±»å¹¶è¦†å†™ç›¸å…³æ–¹æ³•ã€‚

æ‰€è°“æ•°æ®é›†ï¼Œå…¶å®å°±æ˜¯ä¸€ä¸ªè´Ÿè´£å¤„ç†ç´¢å¼•ï¼ˆindexï¼‰åˆ°æ ·æœ¬ï¼ˆsampleï¼‰æ˜ å°„çš„ä¸€ä¸ªç±»ï¼ˆclassï¼‰ã€‚Pytorchæä¾›ä¸¤ç§æ•°æ®é›†ï¼š Map å¼æ•°æ®é›† å’Œ Iterable å¼æ•°æ®é›†ã€‚

### âœ 1.1ã€mapå¼æ•°æ®é›†

ä¸€ä¸ªMapå¼çš„æ•°æ®é›†å¿…é¡»è¦é‡å†™`__getitem__(self, index)`å’Œ`__len__(self)` ä¸¤ä¸ªå†…å»ºæ–¹æ³•ï¼Œç”¨æ¥è¡¨ç¤ºä»ç´¢å¼•åˆ°æ ·æœ¬çš„æ˜ å°„ï¼ˆMapï¼‰ã€‚è¿™æ ·ä¸€ä¸ªæ•°æ®é›†datasetï¼Œå½“ä½¿ç”¨`dataset[idx]`å‘½ä»¤æ—¶ï¼Œå¯ä»¥åœ¨ä½ çš„ç¡¬ç›˜ä¸­è¯»å–ä½ çš„æ•°æ®é›†ä¸­ç¬¬idx å¼ å›¾ç‰‡ä»¥åŠå…¶æ ‡ç­¾ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ï¼Œ`len(dataset)`åˆ™ä¼šè¿”å›è¿™ä¸ªæ•°æ®é›†çš„å®¹é‡ã€‚

è‡ªå®šä¹‰ç±»å¤§è‡´æ˜¯è¿™æ ·çš„ï¼š

```python
class CustomDataset(data.Dataset): # éœ€è¦ç»§æ‰¿data.Dataset
    def __init__(self):
        # TODO
        # 1. Initialize file path or list of file names.
        pass
    def __getitem__(self, index):
        # TODO
        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).
        # 2. Preprocess the data (e.g. torchvision.Transform).
        # 3. Return a data pair (e.g. image and label).
        pass
    def __len__(self):
        # You should change 0 to the total size of your dataset.
        return 0
```

### âœ 1.2ã€iterableå¼æ•°æ®é›†

ä¸€ä¸ª Iterable å¼æ•°æ®é›†æ˜¯æŠ½è±¡ç±» **`torch.utils.data.IterableDataset`** çš„å­ç±»ï¼Œå¹¶ä¸”è¦†å†™äº†**`__iter__`**æ–¹æ³•æˆä¸ºä¸€ä¸ªè¿­ä»£å™¨ã€‚è¿™ç§æ•°æ®é›†ä¸»è¦ç”¨äºæ•°æ®å¤§å°æœªçŸ¥ï¼Œæˆ–è€…ä»¥æµçš„å½¢å¼çš„è¾“å…¥ï¼Œæœ¬åœ°æ–‡ä»¶ä¸å›ºå®šçš„æƒ…å†µï¼Œéœ€è¦ä»¥è¿­ä»£çš„æ–¹å¼æ¥è·å–æ ·æœ¬ç´¢å¼•ã€‚

## ğŸ–Œ 2ã€DataLoader

é€šè¿‡ç»§æ‰¿`torch.utils.data.Dataset`çš„è¿™ä¸ªæŠ½è±¡ç±»ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰å¥½æˆ‘ä»¬éœ€è¦çš„æ•°æ®ç±»ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡è¿­ä»£çš„æ–¹å¼æ¥å–å¾—æ¯ä¸€ä¸ªæ•°æ®ï¼Œä½†æ˜¯è¿™æ ·å¾ˆéš¾å®ç°å–batchï¼Œshuffleæˆ–è€…å¤šçº¿ç¨‹è¯»å–æ•°æ®ï¼Œæ‰€ä»¥pytorchè¿˜æä¾›äº†ä¸€ä¸ªç®€å•çš„æ–¹æ³•æ¥åšè¿™ä»¶äº‹æƒ…ï¼Œé€šè¿‡`torch.utils.data.DataLoader`ç±»æ¥å®šä¹‰ä¸€ä¸ªæ–°çš„è¿­ä»£å™¨ï¼Œç”¨æ¥å°†è‡ªå®šä¹‰çš„æ•°æ®è¯»å–æ¥å£æˆ–è€… PyTorch å·²æœ‰çš„æ•°æ®è¯»å–æ¥å£çš„è¾“å‡ºæŒ‰ç…§ batch size å°è£…æˆTensorï¼Œåç»­åªéœ€è¦å†åŒ…è£…æˆ Variable å³å¯ä½œä¸ºæ¨¡å‹çš„è¾“å…¥ã€‚

 `Dataloader`ç±»æºç å¦‚ä¸‹ï¼š

```python
# torch.utils.data.DataLoader

class DataLoader(object):

    __initialized = False

    def __init__(self, dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None,
                 num_workers=0, collate_fn=default_collate, pin_memory=False, drop_last=False,
                 timeout=0, worker_init_fn=None):
        self.dataset = dataset
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.collate_fn = collate_fn
        self.pin_memory = pin_memory
        self.drop_last = drop_last
        self.timeout = timeout
        self.worker_init_fn = worker_init_fn

        if timeout < 0:
            raise ValueError('timeout option should be non-negative')

        if batch_sampler is not None:
            if batch_size > 1 or shuffle or sampler is not None or drop_last:
                raise ValueError('batch_sampler option is mutually exclusive '
                                 'with batch_size, shuffle, sampler, and '
                                 'drop_last')
            self.batch_size = None
            self.drop_last = None

        if sampler is not None and shuffle:
            raise ValueError('sampler option is mutually exclusive with '
                             'shuffle')

        if self.num_workers < 0:
            raise ValueError('num_workers option cannot be negative; '
                             'use num_workers=0 to disable multiprocessing.')

        if batch_sampler is None:
            if sampler is None:
                if shuffle:
                    # dataset.__len__() åœ¨ Sampler ä¸­è¢«ä½¿ç”¨ã€‚
                    # ç›®çš„æ˜¯ç”Ÿæˆä¸€ä¸ª é•¿åº¦ä¸º len(dataset) çš„ åºåˆ—ç´¢å¼•ï¼ˆéšæœºçš„ï¼‰ã€‚
                    sampler = RandomSampler(dataset)
                else:
                    # dataset.__len__() åœ¨ Sampler ä¸­è¢«ä½¿ç”¨ã€‚
                    # ç›®çš„æ˜¯ç”Ÿæˆä¸€ä¸ª é•¿åº¦ä¸º len(dataset) çš„ åºåˆ—ç´¢å¼•ï¼ˆé¡ºåºçš„ï¼‰ã€‚
                    sampler = SequentialSampler(dataset)
            # Sampler æ˜¯ä¸ªè¿­ä»£å™¨ï¼Œä¸€æ¬¡ä¹‹åªè¿”å›ä¸€ä¸ª ç´¢å¼•
            # BatchSampler ä¹Ÿæ˜¯ä¸ªè¿­ä»£å™¨ï¼Œä½†æ˜¯ä¸€æ¬¡è¿”å› batch_size ä¸ª ç´¢å¼•
            batch_sampler = BatchSampler(sampler, batch_size, drop_last)

        self.sampler = sampler
        self.batch_sampler = batch_sampler
        self.__initialized = True

    def __setattr__(self, attr, val):
        if self.__initialized and attr in ('batch_size', 'sampler', 'drop_last'):
            raise ValueError('{} attribute should not be set after {} is '
                             'initialized'.format(attr, self.__class__.__name__))

        super(DataLoader, self).__setattr__(attr, val)

    def __iter__(self):
        return _DataLoaderIter(self)

    def __len__(self):
        return len(self.batch_sampler)

'''
dataset(Dataset): ä¼ å…¥çš„æ•°æ®é›†
batch_size(int, optional): æ¯ä¸ªbatchæœ‰å¤šå°‘ä¸ªæ ·æœ¬
shuffle(bool, optional): åœ¨æ¯ä¸ªepochå¼€å§‹çš„æ—¶å€™ï¼Œå¯¹æ•°æ®è¿›è¡Œé‡æ–°æ’åºï¼Œè‹¥è¿™ä¸ªå‚æ•°æœ‰å®šä¹‰ï¼Œåˆ™shuffleå¿…é¡»ä¸ºFalse
sampler(Sampler, optional): è‡ªå®šä¹‰ä»æ•°æ®é›†ä¸­å–æ ·æœ¬çš„ç­–ç•¥ï¼Œå¦‚æœæŒ‡å®šè¿™ä¸ªå‚æ•°ï¼Œé‚£ä¹ˆshuffleå¿…é¡»ä¸ºFalse
batch_sampler(Sampler, optional): ä¸samplerç±»ä¼¼ï¼Œä½†æ˜¯ä¸€æ¬¡åªè¿”å›ä¸€ä¸ªbatchçš„indicesï¼ˆç´¢å¼•ï¼‰ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ
       ä¸€æ—¦æŒ‡å®šäº†è¿™ä¸ªå‚æ•°ï¼Œé‚£ä¹ˆbatch_size,shuffle,sampler,drop_lastå°±ä¸èƒ½å†åˆ¶å®šäº†ï¼ˆäº’æ–¥â€”â€”Mutually exclusiveï¼‰
num_workers (int, optional): è¿™ä¸ªå‚æ•°å†³å®šäº†æœ‰å‡ ä¸ªè¿›ç¨‹æ¥å¤„ç†data loadingã€‚0æ„å‘³ç€æ‰€æœ‰çš„æ•°æ®éƒ½ä¼šè¢«loadè¿›ä¸»è¿›ç¨‹ã€‚ï¼ˆé»˜è®¤ä¸º0ï¼‰
collate_fn (callable, optional): å°†ä¸€ä¸ªlistçš„sampleç»„æˆä¸€ä¸ªmini-batchçš„å‡½æ•°
pin_memory (bool, optional)ï¼š å¦‚æœè®¾ç½®ä¸ºTrueï¼Œé‚£ä¹ˆdata loaderå°†ä¼šåœ¨è¿”å›å®ƒä»¬ä¹‹å‰ï¼Œå°†tensorsæ‹·è´åˆ°CUDAä¸­çš„å›ºå®šå†…å­˜ï¼ˆCUDA pinned memoryï¼‰ä¸­
drop_last (bool, optional): å¦‚æœè®¾ç½®ä¸ºTrueï¼šè¿™ä¸ªæ˜¯å¯¹æœ€åçš„æœªå®Œæˆçš„batchæ¥è¯´çš„ï¼Œæ¯”å¦‚ä½ çš„batch_sizeè®¾ç½®ä¸º64ï¼Œ
       è€Œä¸€ä¸ªepochåªæœ‰100ä¸ªæ ·æœ¬ï¼Œé‚£ä¹ˆè®­ç»ƒçš„æ—¶å€™åé¢çš„36ä¸ªå°±è¢«æ‰”æ‰äº†ï¼Œå¦‚æœä¸ºFalseï¼ˆé»˜è®¤ï¼‰ï¼Œé‚£ä¹ˆä¼šç»§ç»­æ­£å¸¸æ‰§è¡Œï¼Œåªæ˜¯æœ€åçš„batch_sizeä¼šå°ä¸€ç‚¹ã€‚
timeout(numeric, optional): å¦‚æœæ˜¯æ­£æ•°ï¼Œè¡¨æ˜ç­‰å¾…ä»workerè¿›ç¨‹ä¸­æ”¶é›†ä¸€ä¸ªbatchç­‰å¾…çš„æ—¶é—´ï¼Œè‹¥è¶…å‡ºè®¾å®šçš„æ—¶é—´è¿˜æ²¡æœ‰æ”¶é›†åˆ°ï¼Œ
       é‚£å°±ä¸æ”¶é›†è¿™ä¸ªå†…å®¹äº†ã€‚è¿™ä¸ªnumericåº”æ€»æ˜¯å¤§äºç­‰äº0ã€‚é»˜è®¤ä¸º0
worker_init_fn (callable, optional): æ¯ä¸ªworkeråˆå§‹åŒ–å‡½æ•°ï¼Œé»˜è®¤ä¸ºNoneï¼Œå¦‚æœä¸æ˜¯Noneï¼Œè¿™ä¸ªå‡½æ•°å°†è¢«æ¯ä¸ªå­è¿›ç¨‹ä»¥å­è¿›ç¨‹idï¼ˆ[0, num_workers - 1]ä¹‹é—´çš„æ•°ï¼‰è°ƒç”¨
'''
```

åœ¨ `DataLoader` ä¸­ï¼Œ`iter(dataloader)` è¿”å›çš„æ˜¯ä¸€ä¸ª `DataLoaderIter` å¯¹è±¡ï¼Œ è¿™ä¸ªæ‰æ˜¯æˆ‘ä»¬ä¸€ç›´ `next`çš„ å¯¹è±¡ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯DataLoaderçš„éƒ¨åˆ†åˆå§‹åŒ–å‚æ•°ä¹‹é—´å­˜åœ¨äº’æ–¥å…³ç³»ï¼š

* å¦‚æœè‡ªå®šä¹‰äº†`batch_sampler`ï¼Œé‚£ä¹ˆè¿™äº›å‚æ•°éƒ½å¿…é¡»ä½¿ç”¨é»˜è®¤å€¼ï¼š`batch_size`ï¼Œ`shuffle`ï¼Œ`sampler`ï¼Œ`drop_last`ã€‚
* å¦‚æœè‡ªå®šä¹‰äº†`sampler`ï¼Œé‚£ä¹ˆ`shuffle`éœ€è¦è®¾ç½®ä¸º`False`ã€‚
* å¦‚æœ`sampler`å’Œ`batch_sampler`éƒ½ä¸º`None`ï¼Œé‚£ä¹ˆ`batch_sampler`ä½¿ç”¨Pytorchå·²ç»å®ç°å¥½çš„`BatchSampler`ï¼Œè€Œ`sampler`åˆ†ä¸¤ç§æƒ…å†µï¼š
  * è‹¥`shuffle=True`ï¼Œåˆ™`sampler=RandomSampler(dataset)`
  * è‹¥`shuffle=False`ï¼Œåˆ™`sampler=SequentialSampler(dataset)`

### âœ 2.1ã€**DataLoaderIter**

**æºç å¦‚ä¸‹ï¼š**

```python
class _DataLoaderIter(object):

    def __init__(self, loader):
        # loader æ˜¯ DataLoader å¯¹è±¡
        self.dataset = loader.dataset
        self.collate_fn = loader.collate_fn
        self.batch_sampler = loader.batch_sampler
        self.num_workers = loader.num_workers
        self.pin_memory = loader.pin_memory and torch.cuda.is_available()
        self.timeout = loader.timeout
        # è¿™æ ·å°±å¯ä»¥ç”¨ next æ“ä½œ batch_sampler äº†
        self.sample_iter = iter(self.batch_sampler)

        base_seed = torch.LongTensor(1).random_().item()

        if self.num_workers > 0:
            self.worker_init_fn = loader.worker_init_fn
            self.worker_queue_idx = 0
            self.worker_result_queue = multiprocessing.Queue()
            
            # å½“å‰å·²ç»å‡†å¤‡å¥½çš„ batch çš„æ•°é‡ï¼ˆå¯èƒ½æœ‰äº›æ­£åœ¨ å‡†å¤‡ä¸­ï¼‰
            # å½“ä¸º 0 æ—¶ï¼Œ è¯´æ˜ï¼Œ dataset ä¸­å·²ç»æ²¡æœ‰å‰©ä½™æ•°æ®äº†ã€‚
            # åˆå§‹å€¼ä¸º 0, åœ¨ self._put_indices() ä¸­ +1,åœ¨ self.__next__ ä¸­å‡ä¸€
            self.batches_outstanding = 0
            self.worker_pids_set = False
            self.shutdown = False
            
            # ç”¨æ¥è®°å½• è¿™æ¬¡è¦æ”¾åˆ° index_queue ä¸­ batch çš„ idx
            self.send_idx = 0
            
            # ç”¨æ¥è®°å½• è¿™æ¬¡è¦ä»çš„ data_queue ä¸­å–å‡º çš„ batch çš„ idx
            self.rcvd_idx = 0
            self.reorder_dict = {}
            self.done_event = multiprocessing.Event()

            self.index_queues = []
            self.workers = []
            for i in range(self.num_workers):
                
                index_queue = multiprocessing.Queue()
                index_queue.cancel_join_thread()
                w = multiprocessing.Process(
                    target=_utils.worker._worker_loop,
                    args=(self.dataset, index_queue,
                          self.worker_result_queue, self.done_event,
                          self.collate_fn, base_seed + i,
                          self.worker_init_fn, i))
                w.daemon = True

                w.start()
                self.index_queues.append(index_queue)
                self.workers.append(w)

            if self.pin_memory:
                self.data_queue = queue.Queue()
                pin_memory_thread = threading.Thread(
                    target=_utils.pin_memory._pin_memory_loop,
                    args=(self.worker_result_queue, self.data_queue,
                          torch.cuda.current_device(), self.done_event))
                pin_memory_thread.daemon = True
                pin_memory_thread.start()
                # Similar to workers (see comment above), we only register
                # pin_memory_thread once it is started.
                self.pin_memory_thread = pin_memory_thread
            else:
                self.data_queue = self.worker_result_queue

            _utils.signal_handling._set_worker_pids(id(self), tuple(w.pid for w in self.workers))
            _utils.signal_handling._set_SIGCHLD_handler()
            self.worker_pids_set = True

            # prime the prefetch loop
            for _ in range(2 * self.num_workers):
                self._put_indices()

    def __len__(self):
        return len(self.batch_sampler)

    def _try_get_batch(self, timeout=_utils.MP_STATUS_CHECK_INTERVAL):
        # Tries to fetch data from `data_queue` for a given timeout. This can
        # also be used as inner loop of fetching without timeout, with the
        # sender status as the loop condition.
        #
        # This raises a `RuntimeError` if any worker died expectedly. This error
        # can come from either the SIGCHLD handler in `_utils/signal_handling.py`
        # (only for non-Windows platforms), or the manual check below on errors
        # and timeouts.
        #
        # Returns a 2-tuple:
        #   (bool: whether successfully get data, any: data if successful else None)
        try:
            data = self.data_queue.get(timeout=timeout)
            return (True, data)
        except Exception as e:
            if not all(w.is_alive() for w in self.workers):
                pids_str = ', '.join(str(w.pid) for w in self.workers if not w.is_alive())
                raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str))
            if isinstance(e, queue.Empty):
                return (False, None)
            raise
    
    '''
    ä¸»è¦æ ¹æ®æ˜¯å¦è®¾ç½®äº†è¶…æ—¶æ—¶é—´æ¥æ“ä½œï¼Œå¦‚æœè¶…è¿‡æŒ‡å®šçš„è¶…æ—¶æ—¶é—´åæ²¡æœ‰ä»é˜Ÿåˆ—ä¸­è¯»åˆ°æ•°æ®å°±æŠ¥é”™ï¼Œ
    å¦‚æœä¸è®¾ç½®è¶…æ—¶æ—¶é—´ä¸”ä¸€è‡´æ²¡æœ‰ä»é˜Ÿåˆ—ä¸­è¯»åˆ°æ•°æ®ï¼Œé‚£ä¹ˆå°±ä¼šä¸€ç›´å¡ç€ä¸”ä¸æŠ¥é”™ï¼Œè¿™éƒ¨åˆ†æ˜¯PyTorchåæ¥ä¿®çš„ä¸€ä¸ªbugã€‚
    '''
    def _get_batch(self):
        # Fetches data from `self.data_queue`.
        #
        # We check workers' status every `MP_STATUS_CHECK_INTERVAL` seconds,
        # which we achieve by running `self._try_get_batch(timeout=MP_STATUS_CHECK_INTERVAL)`
        # in a loop. This is the only mechanism to detect worker failures for
        # Windows. For other platforms, a SIGCHLD handler is also used for
        # worker failure detection.
        #
        # If `pin_memory=True`, we also need check if `pin_memory_thread` had
        # died at timeouts.
        if self.timeout > 0:
            success, data = self._try_get_batch(self.timeout)
            if success:
                return data
            else:
                raise RuntimeError('DataLoader timed out after {} seconds'.format(self.timeout))
        elif self.pin_memory:
            while self.pin_memory_thread.is_alive():
                success, data = self._try_get_batch()
                if success:
                    return data
            else:
                # while condition is false, i.e., pin_memory_thread died.
                raise RuntimeError('Pin memory thread exited unexpectedly')
            # In this case, `self.data_queue` is a `queue.Queue`,. But we don't
            # need to call `.task_done()` because we don't use `.join()`.
        else:
            while True:
                success, data = self._try_get_batch()
                if success:
                    return data

    def __next__(self):
        if self.num_workers == 0:  # same-process loading
            indices = next(self.sample_iter)  # may raise StopIteration
            batch = self.collate_fn([self.dataset[i] for i in indices])
            if self.pin_memory:
                batch = _utils.pin_memory.pin_memory_batch(batch)
            return batch

        # check if the next sample has already been generated
        if self.rcvd_idx in self.reorder_dict:
            batch = self.reorder_dict.pop(self.rcvd_idx)
            return self._process_next_batch(batch)

        if self.batches_outstanding == 0:
            self._shutdown_workers()
            raise StopIteration

        while True:
            assert (not self.shutdown and self.batches_outstanding > 0)
            idx, batch = self._get_batch()
            self.batches_outstanding -= 1
            if idx != self.rcvd_idx:
                # store out-of-order samples
                self.reorder_dict[idx] = batch
                continue
            return self._process_next_batch(batch)

    next = __next__  # Python 2 compatibility

    def __iter__(self):
        return self
        
    '''
    ä»self.sample_iterä¸­è¯»å–ä¸‹ä¸€ä¸ªbatchæ•°æ®ä¸­æ¯ä¸ªæ•°æ®çš„indexï¼šindices = next(self.sample_iter, None)ï¼Œ
    æ³¨æ„è¿™é‡Œçš„indexå’Œå‰é¢idxæ˜¯ä¸ä¸€æ ·çš„ï¼Œè¿™é‡Œçš„indexæ˜¯ä¸€ä¸ªbatchä¸­æ¯ä¸ªæ•°æ®çš„indexï¼Œidxæ˜¯ä¸€ä¸ªbatchçš„indexï¼›
    ç„¶åå°†è¯»å–åˆ°çš„indexé€šè¿‡è°ƒç”¨queueå¯¹è±¡çš„putæ–¹æ³•å‹åˆ°é˜Ÿåˆ—self.index_queueä¸­ï¼šself.index_queue.put((self.send_idx, indices))
    '''
    def _put_indices(self):
        assert self.batches_outstanding < 2 * self.num_workers
        indices = next(self.sample_iter, None)
        if indices is None:
            return
        self.index_queues[self.worker_queue_idx].put((self.send_idx, indices))
        self.worker_queue_idx = (self.worker_queue_idx + 1) % self.num_workers
        self.batches_outstanding += 1
        self.send_idx += 1

    '''
    é¦–å…ˆå¯¹self.rcvd_idxè¿›è¡ŒåŠ ä¸€ï¼Œä¹Ÿå°±æ˜¯æ›´æ–°ä¸‹ä¸‹ä¸€ä¸ªè¦è¯»å–çš„batchæ•°æ®çš„indexã€‚
    ç„¶åè°ƒç”¨_put_indices()æ–¹æ³•è·å–ä¸‹ä¸€ä¸ªbatchçš„æ¯ä¸ªæ•°æ®çš„indexã€‚
    '''
    def _process_next_batch(self, batch):
        self.rcvd_idx += 1
        self._put_indices()
        if isinstance(batch, _utils.ExceptionWrapper):
            # make multiline KeyError msg readable by working around
            # a python bug https://bugs.python.org/issue2651
            if batch.exc_type == KeyError and "\n" in batch.exc_msg:
                raise Exception("KeyError:" + batch.exc_msg)
            else:
                raise batch.exc_type(batch.exc_msg)
        return batch

    def __getstate__(self):
        # TODO: add limited pickling support for sharing an iterator
        # across multiple threads for HOGWILD.
        # Probably the best way to do this is by moving the sample pushing
        # to a separate thread and then just sharing the data queue
        # but signalling the end is tricky without a non-blocking API
        raise NotImplementedError("_DataLoaderIter cannot be pickled")

    def _shutdown_workers(self):
        # See NOTE [ Data Loader Multiprocessing Shutdown Logic ] for details on
        # the logic of this function.
        python_exit_status = _utils.python_exit_status
        if python_exit_status is True or python_exit_status is None:
            # See (2) of the note. If Python is shutting down, do no-op.
            return
        # Normal exit when last reference is gone / iterator is depleted.
        # See (1) and the second half of the note.
        if not self.shutdown:
            self.shutdown = True
            try:
                self.done_event.set()

                if hasattr(self, 'pin_memory_thread'):
                    # Use hasattr in case error happens before we set the attribute.
                    # First time do `worker_result_queue.put` in this process.

                    # `cancel_join_thread` in case that `pin_memory_thread` exited.
                    self.worker_result_queue.cancel_join_thread()
                    self.worker_result_queue.put(None)
                    self.pin_memory_thread.join()
                    
                    self.worker_result_queue.close()

                # Exit workers now.
                for q in self.index_queues:
                    q.put(None)
                    # Indicate that no more data will be put on this queue by the
                    # current process.
                    q.close()
                for w in self.workers:
                    w.join()
            finally:
                if self.worker_pids_set:
                    _utils.signal_handling._remove_worker_pids(id(self))
                    self.worker_pids_set = False

    def __del__(self):
        if self.num_workers > 0:
            self._shutdown_workers()
```

### âœ 2.2ã€DataLoaderã€DataSetã€Samplerä¹‹é—´çš„å…³ç³»

 çœ‹ä¸€ä¸‹`_DataLoaderIter.__next__()`çš„æºä»£ç ï¼š

```python
class DataLoader(object):
    ...
    
    def __next__(self):
        if self.num_workers == 0:  
            indices = next(self.sample_iter)  # Sampler
            batch = self.collate_fn([self.dataset[i] for i in indices]) # Dataset
            if self.pin_memory:
                batch = _utils.pin_memory.pin_memory_batch(batch)
            return batch
```

å‡è®¾æˆ‘ä»¬çš„æ•°æ®æ˜¯ä¸€ç»„å›¾åƒï¼Œæ¯ä¸€å¼ å›¾åƒå¯¹åº”ä¸€ä¸ªindexï¼Œé‚£ä¹ˆå¦‚æœæˆ‘ä»¬è¦è¯»å–æ•°æ®å°±åªéœ€è¦å¯¹åº”çš„indexå³å¯ï¼Œå³ä¸Šé¢ä»£ç ä¸­çš„`indices`ï¼Œè€Œé€‰å–indexçš„æ–¹å¼æœ‰å¤šç§ï¼Œæœ‰æŒ‰é¡ºåºçš„ï¼Œä¹Ÿæœ‰ä¹±åºçš„ï¼Œæ‰€ä»¥è¿™ä¸ªå·¥ä½œéœ€è¦`Sampler`å®Œæˆã€‚æ‹¿åˆ°äº†`indices`ï¼Œä¸‹ä¸€æ­¥åªéœ€è¦æ ¹æ®indexå¯¹æ•°æ®è¿›è¡Œè¯»å–å³å¯ã€‚

å‚æ•°çš„æ„ä¹‰ï¼š

* `indices`: è¡¨ç¤ºæ¯ä¸€ä¸ªiterationï¼Œsamplerè¿”å›çš„indicesï¼Œå³ä¸€ä¸ªbatch sizeå¤§å°çš„ç´¢å¼•åˆ—è¡¨
* `self.dataset[i]`: å‰é¢å·²ç»ä»‹ç»äº†ï¼Œè¿™é‡Œå°±æ˜¯å¯¹ç¬¬iä¸ªæ•°æ®è¿›è¡Œè¯»å–æ“ä½œï¼Œä¸€èˆ¬æ¥è¯´`self.dataset[i]=(img, label)`

å‰é¢è¿˜æœ‰ä¸€ä¸ª`self.collate_fn`æ–¹æ³•ï¼Œå…¶ä½œç”¨å°±æ˜¯å°†ä¸€ä¸ªbatchçš„æ•°æ®è¿›è¡Œåˆå¹¶æ“ä½œã€‚é»˜è®¤çš„`collate_fn`æ˜¯å°†imgå’Œlabelåˆ†åˆ«åˆå¹¶æˆimgså’Œlabelsï¼Œæ‰€ä»¥å¦‚æœä½ çš„`__getitem__`æ–¹æ³•åªæ˜¯è¿”å› `img, label`ï¼Œé‚£ä¹ˆä½ å¯ä»¥ä½¿ç”¨é»˜è®¤çš„`collate_fn`æ–¹æ³•ï¼Œä½†æ˜¯å¦‚æœä½ æ¯æ¬¡è¯»å–çš„æ•°æ®æœ‰`img, box, label`ç­‰ç­‰ï¼Œé‚£ä¹ˆä½ å°±éœ€è¦è‡ªå®šä¹‰`collate_fn`æ¥å°†å¯¹åº”çš„æ•°æ®åˆå¹¶æˆä¸€ä¸ªbatchæ•°æ®ï¼Œè¿™æ ·æ–¹ä¾¿åç»­çš„è®­ç»ƒæ­¥éª¤ã€‚

```python
def default_collate(batch):
    "Puts each data field into a tensor with outer dimension batch size"
    if torch.is_tensor(batch[0]):
        out = None
        if _use_shared_memory:
            # If we're in a background process, concatenate directly into a
            # shared memory tensor to avoid an extra copy
            # è®¡ç®— batch ä¸­æ‰€æœ‰ å…ƒç´ çš„ä¸ªæ•° 
            numel = sum([x.numel() for x in batch])
            storage = batch[0].storage()._new_shared(numel)
            out = batch[0].new(storage)
        return torch.stack(batch, 0, out=out)
    elif type(batch[0]).__module__ == 'numpy':
        elem = batch[0]
        if type(elem).__name__ == 'ndarray':
            return torch.stack([torch.from_numpy(b) for b in batch], 0)
        if elem.shape == ():  # scalars
            py_type = float if elem.dtype.name.startswith('float') else int
            return numpy_type_map[elem.dtype.name](list(map(py_type, batch)))
    elif isinstance(batch[0], int):
        return torch.LongTensor(batch)
    elif isinstance(batch[0], float):
        return torch.DoubleTensor(batch)
    elif isinstance(batch[0], string_classes):
        return batch
    elif isinstance(batch[0], collections.Mapping):
        return {key: default_collate([d[key] for d in batch]) for key in batch[0]}
    elif isinstance(batch[0], collections.Sequence):
        transposed = zip(*batch)
        return [default_collate(samples) for samples in transposed]

    raise TypeError(("batch must contain tensors, numbers, dicts or lists; found {}"
                     .format(type(batch[0]))))
```

{% hint style="info" %}
å¦‚æœåœ¨ `__getitem__()` æˆ– `collate_fn()` ä¸­ä½¿ç”¨äº† `numpy` äº§ç”Ÿéšæœºæ•°ï¼Œ åˆ™éœ€è¦æ³¨æ„ï¼š [why-does-numpy-random-rand-produce-the-same-values-in-different-cores](https://discuss.pytorch.org/t/why-does-numpy-random-rand-produce-the-same-values-in-different-cores/12005) ï¼Œå¦‚æœä½¿ç”¨çš„æ˜¯ `torch` äº§ç”Ÿéšæœºæ•°ï¼Œ å°±ä¸ç”¨æ‹…å¿ƒè¿™ä¸€ç‚¹ã€‚
{% endhint %}

